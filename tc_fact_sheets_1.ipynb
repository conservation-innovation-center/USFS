{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import arcpy\n",
    "import csv\n",
    "import traceback\n",
    "import logging\n",
    "\n",
    "def set_years(cf):\n",
    "    lu_tables = r\"M:\\code\\landuse\\lookup_tables\"\n",
    "    lc_dates = f\"{lu_tables}/landcover_dates.csv\" # T1 and T2 by cofips\n",
    "\n",
    "    # read in years for county\n",
    "    dates = pd.read_csv(lc_dates)\n",
    "    dates = dates.set_index('co_fips')\n",
    "    global T1_yr\n",
    "    T1_yr = int(dates.loc[cf, 'T1'])\n",
    "    global T2_yr\n",
    "    T2_yr = int(dates.loc[cf, 'T2'])\n",
    "    del dates\n",
    "    return T1_yr, T2_yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID = '24001'\n"
     ]
    }
   ],
   "source": [
    "# # cf = \"kent_10001\"\n",
    "# FIPS = cf.split('_')[1]\n",
    "# T1,T2 = set_years(cf)\n",
    "# tempDir = f\"M:/projects/tc_fact_sheet/data/temp\"\n",
    "# coDir = f\"M:/projects/tc_fact_sheet/data/prod/{cf}\"\n",
    "# CoBoundsPath = r\"J:\\GIS\\CBP_Obj_1\\data\\county_sets\\tl_2017_bay_county.shp\"\n",
    "\n",
    "# if not os.path.isdir(coDir):\n",
    "#     os.mkdir(coDir)\n",
    "\n",
    "\n",
    "# CoSel = os.path.join(tempDir,f\"{cf}_bounds.shp\")\n",
    "\n",
    "# where_clause = f\"\"\"GEOID = '{FIPS}'\"\"\"\n",
    "# print(where_clause)\n",
    "\n",
    "# BayHexPath = r\"M:\\projects\\data_processing\\data_processing.gdb\\baywide_1000ac\"\n",
    "# CoHexPath = os.path.join(coDir,f\"{cf}_hex.shp\")\n",
    "# calcCSVPath = os.path.join(coDir,f\"{cf}_ta_calc.csv\")\n",
    "# ta_dbf_path = os.path.join(coDir,f\"{cf}_hex_ta.dbf\")\n",
    "# ta_xlsx_path = ta_dbf_path.replace(\".dbf\",\".xlsx\")\n",
    "# lulc_chgPath = f\"B:/landuse/version2/{cf}/output/{cf}_landusechange_{T1}{T2}.tif\"\n",
    "# t2luPath = f\"B:/landuse/version2/{cf}/output/{cf}_lu_{T2}.tif\"\n",
    "\n",
    "# tadf = pd.read_excel(ta_xlsx_path)\n",
    "\n",
    "# tc = [24,25,26,27,41,42]\n",
    "# dev = [21,22,23,28,29,33,34,35,36,37,38,51,52,53]\n",
    "# #dev includes solar and extractive lands excluding harvested forest.\n",
    "# wat = [11]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss classes 47\n",
      "gain classes 17\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# tadf['tc_chg'] = 0\n",
    "\n",
    "# tavals = [col for col in tadf.columns if col[0:5] == \"VALUE\"]\n",
    "\n",
    "# loss_vals = []\n",
    "# gain_vals = []\n",
    "# wat_vals = []\n",
    "# all_vals = []\n",
    "# # for tv in list(tadf.VALUE.unique()):\n",
    "# for tv in tavals:\n",
    "# ## If it contains the VALUE prefix use this section\n",
    "#     T1val = int(tv[6:8])\n",
    "#     T2val = int(tv[8:10])\n",
    "\n",
    "#     if T1val in tc and T2val in dev:\n",
    "#         loss_vals.append(tv)\n",
    "    \n",
    "#     if T1val in dev and T2val in tc:\n",
    "#         gain_vals.append(tv)\n",
    "    \n",
    "#     if T1val in wat or T2val in wat: # OR\n",
    "#         wat_vals.append(tv)\n",
    "\n",
    "#     if \"VALUE_\" in tv: # OR\n",
    "#         all_vals.append(tv)\n",
    "\n",
    "# print(\"loss classes\",len(loss_vals))\n",
    "# print(\"gain classes\",len(gain_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tadf['tc_loss']=tadf[loss_vals].sum(axis=1)\n",
    "# tadf['tc_gain']=tadf[gain_vals].sum(axis=1)\n",
    "# tadf['tc_chg']=tadf['tc_gain']-tadf['tc_loss']\n",
    "\n",
    "# # tadf['wat']=tadf[wat_vals].sum(axis=1)\n",
    "# # tadf['all_vals']=tadf[all_vals].sum(axis=1)\n",
    "\n",
    "# tadf.hist('tc_chg')\n",
    "\n",
    "\n",
    "# # tadf_clean = tadf[['GRID_ID','tc_chg','tc_gain','tc_loss','wat','all_vals']]\n",
    "# tadf_clean = tadf[['GRID_ID','tc_chg','tc_gain','tc_loss']]\n",
    "# tadf_clean.to_csv(calcCSVPath.replace('.csv','test.csv'),index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #SUM WATER VALUES AND JOIN TO SHP WITH GPD +save xlsx as csv \n",
    "# lcdates_csv = r\"M:\\code\\landuse\\lookup_tables\\landcover_dates.csv\"\n",
    "# co_list = list(csv.reader(open(lcdates_csv, \"r\"), delimiter=\",\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# elist = [] #clear elist\n",
    "# for idx in co_list[2:]: # skip header and DC\n",
    "#     cf = idx[4] \n",
    "#     print(f\"{cf} {idx}\", end='   \\r')\n",
    "\n",
    "#     try:\n",
    "#         T1,T2 = set_years(cf)\n",
    "#         coDir = f\"M:/projects/tc_fact_sheet/data/prod/{cf}\"\n",
    "#         CoHexPath_joined2 = os.path.join(coDir,f\"{cf}_hex_tajoined2.shp\")\n",
    "#         t2luPath = f\"B:/landuse/version2/{cf}/output/{cf}_lu_{T2}.tif\"\n",
    "#         t2_dbf_path = os.path.join(coDir,f\"{cf}_hex_t2_ta.dbf\")\n",
    "#         t2_xlsx_path = t2_dbf_path.replace(\".dbf\",\".xlsx\")\n",
    "#         t2_csv_path = t2_xlsx_path.replace('xlsx','csv')\n",
    "#         tajoined3Path = CoHexPath_joined2.replace(\"tajoined2\",\"tajoined3\")\n",
    "#         if not os.path.exists(t2_csv_path):  \n",
    "#             t2df = pd.read_excel(t2_xlsx_path)\n",
    "#             t2vals = [col for col in t2df.columns if col[0:5] == \"VALUE\"]\n",
    "#             wat_vals = [\"VALUE_11\",\"VALUE_12\",\"VALUE_13\",\"VALUE_14\",\"VALUE_15\"]\n",
    "#             t2df['wat_lu'] = t2df[wat_vals].sum(axis=1)\n",
    "#             t2df['all_lu']= t2df[t2vals].sum(axis=1)\n",
    "#             t2df.to_csv(t2_csv_path)\n",
    "#         else:\n",
    "#             t2df = pd.read_excel(t2_xlsx_path)\n",
    "#     except:\n",
    "#         elist.append(f\"{cf} - xlsx df, calculate, csv, conversion\")\n",
    "#     try:\n",
    "#         # if not os.path.exists(tajoined3Path):  \n",
    "#         t2df = pd.read_excel(t2_xlsx_path)\n",
    "#         hex_df = gpd.read_file(CoHexPath_joined2)\n",
    "#         hex_df.merge(t2df,how=\"inner\",left_on=\"GRID_ID\",right_on=\"GRID_ID\")\n",
    "#         hex_df.to_file(tajoined3Path)\n",
    "#     except:\n",
    "#         elist.append(f\"{cf} - GPD merge\")\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_list_csv = r\"M:\\projects\\tc_fact_sheet\\data\\county_list.csv\"\n",
    "cf_list = []\n",
    "with open(county_list_csv, newline='') as inputfile:\n",
    "    for row in csv.reader(inputfile):\n",
    "        cf_list.append(row[0])\n",
    "\n",
    "cf_list.remove(\"bedf_51515\")\n",
    "cf_list.remove(\"dist_11001\")\n",
    "\n",
    "cf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "york_51199 203   \r"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx,cf in enumerate(cf_list):\n",
    "\n",
    "    T1,T2 = set_years(cf)\n",
    "    coDir = f\"M:/projects/tc_fact_sheet/data/prod/{cf}\"\n",
    "    CoHexPath_joined2 = os.path.join(coDir,f\"{cf}_hex_tajoined2.shp\")\n",
    "    t2luPath = f\"B:/landuse/version2/{cf}/output/{cf}_lu_{T2}.tif\"\n",
    "    t2_dbf_path = os.path.join(coDir,f\"{cf}_hex_t2_ta.dbf\")\n",
    "    t2_xlsx_path = t2_dbf_path.replace(\".dbf\",\".xlsx\")\n",
    "    t2_csv_path = t2_xlsx_path.replace('xlsx','csv')\n",
    "    tajoined3Path = CoHexPath_joined2.replace(\"tajoined2\",\"tajoined3\")\n",
    "    print(f\"{cf} {idx}\", end='   \\r')\n",
    "\n",
    "    t2df = pd.read_excel(t2_xlsx_path)\n",
    "    hex_df = gpd.read_file(tajoined3Path)\n",
    "    orig_cols = ['Shape_Leng', 'Shape_Area', 'GRID_ID', 'tc_chg','tc_gain', 'tc_loss','geometry']\n",
    "    hex_df = hex_df[orig_cols]\n",
    "    if len(hex_df.columns) > 12:\n",
    "        print(cf)\n",
    "        print(hex_df.columns)\n",
    "        break\n",
    "\n",
    "    t2vals = [col for col in t2df.columns if col[0:5] == \"VALUE\"]\n",
    "    wat_vals = [\"VALUE_11\",\"VALUE_12\",\"VALUE_13\",\"VALUE_14\",\"VALUE_15\"]\n",
    "    temp_wat_vals = wat_vals\n",
    "    for t2 in wat_vals:\n",
    "        if not t2 in t2vals:\n",
    "            t2df[t2] = 0\n",
    "            # print(f\"{t2} does not exist in {cf} ta table\")\n",
    "\n",
    "    t2df['wat_lu'] = t2df[temp_wat_vals].sum(axis=1)\n",
    "    t2df['all_lu']= t2df[t2vals].sum(axis=1)\n",
    "\n",
    "    hex_df = hex_df.merge(t2df,how=\"inner\",left_on=\"GRID_ID\",right_on=\"GRID_ID\")\n",
    "    hex_df.to_file(tajoined3Path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shape_Leng', 'Shape_Area', 'GRID_ID', 'Unnamed_ 0', 'tc_chg', 'tc_gain', 'tc_loss', 'ALLWAT', 'geometry']\n"
     ]
    }
   ],
   "source": [
    "# \"\"\" CLEANING UP DOUBLE JOIN on KENT_10001\"\"\"\n",
    "# hex_df = gpd.read_file(CoHexPath_joined2)\n",
    "# keeps = list(hex_df.columns) # create list\n",
    "\n",
    "# for col in list(hex_df.columns):\n",
    "#     if \"_x\" in col or \"_y\" in col:\n",
    "#         keeps.remove(col)\n",
    "# keeps.remove(\"wat_lu\")\n",
    "# keeps.remove(\"all_lu\")\n",
    "\n",
    "# print(keeps)\n",
    "# cleaned = hex_df[keeps]\n",
    "# cleaned.to_file(CoHexPath_joined2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOID = '51001'\n"
     ]
    }
   ],
   "source": [
    "# for cf in cflist:\n",
    "\n",
    "#     FIPS = cf.split('_')[1]\n",
    "#     T1,T2 = set_years(cf)\n",
    "#     # T1 = 2014\n",
    "#     # T2 = 2018\n",
    "\n",
    "#     tempDir = f\"M:/projects/tc_fact_sheet/data/temp\"\n",
    "#     coDir = f\"M:/projects/tc_fact_sheet/data/prod/{cf}\"\n",
    "#     CoBoundsPath = r\"J:\\GIS\\CBP_Obj_1\\data\\county_sets\\tl_2017_bay_county.shp\"\n",
    "\n",
    "#     if not os.path.isdir(coDir):\n",
    "#         os.mkdir(coDir)\n",
    "\n",
    "\n",
    "#     CoSel = os.path.join(tempDir,f\"{cf}_bounds.shp\")\n",
    "\n",
    "#     where_clause = f\"\"\"GEOID = '{FIPS}'\"\"\"\n",
    "#     print(where_clause)\n",
    "\n",
    "#     BayHexPath = r\"M:\\projects\\data_processing\\data_processing.gdb\\baywide_1000ac\"\n",
    "#     CoHexPath = os.path.join(coDir,f\"{cf}_hex.shp\")\n",
    "#     calcCSVPath = os.path.join(coDir,f\"{cf}_ta_calc.csv\")\n",
    "#     ta_dbf_path = os.path.join(coDir,f\"{cf}_hex_ta.dbf\")\n",
    "#     ta_xlsx_path = ta_dbf_path.replace(\".dbf\",\".xlsx\")\n",
    "#     lulc_chgPath = f\"B:/landuse/version2/{cf}/output/{cf}_landusechange_{T1}{T2}.tif\"\n",
    "\n",
    "#     if not arcpy.Exists(CoSel):\n",
    "#         sel_temp = arcpy.management.SelectLayerByAttribute(CoBoundsPath, \"NEW_SELECTION\", where_clause)\n",
    "#         arcpy.FeatureClassToFeatureClass_conversion(sel_temp, tempDir, os.path.basename(CoSel))\n",
    "#     else:\n",
    "#         print(f\"{CoSel} exists.\")\n",
    "\n",
    "#     \"\"\"\n",
    "# if COUNTY SIZE < ??:\n",
    "#     BayHexPath = r\"SMALL_HEXS\"\n",
    "# else:\n",
    "#     BayHexPath = r\"BIG_HEXS\"\n",
    "# \"\"\"\n",
    "#     tempHex = arcpy.SelectLayerByLocation_management(BayHexPath, 'INTERSECT', CoSel)\n",
    "#     arcpy.FeatureClassToFeatureClass_conversion(tempHex, coDir, os.path.basename(CoHexPath))\n",
    "    \n",
    "    \n",
    "    \n",
    "#     if os.path.exists(lulc_chgPath): \n",
    "#         pass\n",
    "#     else:\n",
    "#         print(f'could not find {lulc_chgPath}')\n",
    "\n",
    "#     arcpy.sa.TabulateArea(CoHexPath, \n",
    "#                           \"GRID_ID\", \n",
    "#                           lulc_chgPath, \n",
    "#                           \"Value\", \n",
    "#                           ta_dbf_path, \n",
    "#                           lulc_chgPath, \n",
    "#                           \"CLASSES_AS_FIELDS\")\n",
    "\n",
    "#     # arcpy.sa.TabulateArea(\"acco_51001_hex\",\n",
    "#     #                       \"GRID_ID\",\n",
    "#     #                       \"bay_lu.tif\", \n",
    "#     #                       \"Value\", \n",
    "#     #                       r\"M:\\projects\\data_processing\\data_processing.gdb\\Tabulat_acco_511\",\n",
    "#     #                       r\"B:\\landuse\\version2_mosaics\\Baywide\\bay_lu.tif\", \n",
    "#     #                       \"CLASSES_AS_FIELDS\")\n",
    "\n",
    "#     # Convert to excel format to read into pandas\n",
    "#     arcpy.conversion.TableToExcel(ta_dbf_path, ta_xlsx_path, \"NAME\", \"CODE\")\n",
    "    \n",
    "#     tadf = pd.read_excel(ta_xlsx_path)\n",
    "\n",
    "#     tc = [24,25,26,27,41,42]\n",
    "#     dev = [21,22,23,28,29,33,34,35,36,37,38,51,52,53]\n",
    "#     #dev includes solar and extractive lands excluding harvested forest.\n",
    "\n",
    "\n",
    "#     tadf['tc_chg'] = 0\n",
    "\n",
    "#     tavals = [col for col in tadf.columns if col[0:5] == \"VALUE\"]\n",
    "\n",
    "#     loss_vals = []\n",
    "#     gain_vals = []\n",
    "#     # for tv in list(tadf.VALUE.unique()):\n",
    "#     for tv in tavals:\n",
    "#     ## If it contains the VALUE prefix use this section\n",
    "#         T1val = int(tv[6:8])\n",
    "#         T2val = int(tv[8:10])\n",
    "#     #     T1val = int(str(tv)[0:2])\n",
    "#     #     T2val = int(str(tv)[2:4])\n",
    "#         if T1val in tc and T2val in dev:\n",
    "#             loss_vals.append(tv)\n",
    "#         elif T1val in dev and T2val in tc:\n",
    "#             gain_vals.append(tv)\n",
    "#         else:\n",
    "#             pass\n",
    "\n",
    "#     print(loss_vals)\n",
    "#     print(gain_vals)\n",
    "\n",
    "#     tadf['tc_loss']=tadf[loss_vals].sum(axis=1)\n",
    "#     tadf['tc_gain']=tadf[gain_vals].sum(axis=1)\n",
    "#     tadf['tc_chg']=tadf['tc_gain']-tadf['tc_loss']\n",
    "    \n",
    "#     tadf_clean = tadf[['GRID_ID','tc_chg','tc_gain','tc_loss']]\n",
    "#     tadf_clean.to_csv(calcCSVPath,index=True)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M:\\\\projects\\\\data_processing'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['County', 'State', 'T1', 'T2', 'co_fips', 'Legend', 'Size', 'NOTE'], dtype='object')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lcdates_csv = r\"M:\\projects\\tc_fact_sheet\\data\\county_list_legendloc.csv\"\n",
    "codf = pd.read_csv(lcdates_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r\n",
      "b\n",
      "r\n",
      "b\n",
      "r\n",
      "b\n",
      "r\n",
      "l\n",
      "l\n",
      "l\n",
      "l\n",
      "l\n",
      "l\n",
      "l\n",
      "l\n",
      "r\n",
      "r\n",
      "r\n",
      "l\n",
      "r\n",
      "l\n",
      "r\n",
      "r\n",
      "b\n",
      "b\n",
      "r\n",
      "r\n",
      "l\n",
      "r\n",
      "l\n",
      "b\n",
      "r\n",
      "l\n",
      "r\n",
      "l\n",
      "r\n",
      "l\n",
      "r\n",
      "l\n",
      "l\n",
      "r\n",
      "r\n",
      "b\n",
      "b\n",
      "b\n",
      "r\n",
      "r\n",
      "r\n",
      "r\n",
      "l\n",
      "l\n",
      "l\n",
      "b\n",
      "b\n",
      "r\n",
      "r\n",
      "r\n",
      "r\n",
      "l\n",
      "r\n",
      "b\n",
      "r\n",
      "r\n",
      "r\n",
      "l\n",
      "r\n",
      "l\n",
      "b\n",
      "l\n",
      "r\n",
      "b\n",
      "r\n",
      "b\n",
      "b\n",
      "r\n",
      "r\n",
      "r\n",
      "l\n",
      "b\n",
      "l\n",
      "r\n",
      "b\n",
      "b\n",
      "r\n",
      "l\n",
      "r\n",
      "r\n",
      "r\n",
      "l\n",
      "r\n",
      "b\n",
      "r\n",
      "l\n",
      "r\n",
      "b\n",
      "r\n",
      "l\n",
      "b\n",
      "r\n",
      "r\n",
      "r\n",
      "r\n",
      "l\n",
      "l\n",
      "l\n",
      "l\n",
      "l\n",
      "b\n",
      "r\n",
      "r\n",
      "r\n",
      "b\n",
      "l\n",
      "b\n",
      "r\n",
      "r\n",
      "r\n",
      "b\n",
      "l\n",
      "r\n",
      "l\n",
      "l\n",
      "b\n",
      "r\n",
      "r\n",
      "r\n",
      "r\n",
      "l\n",
      "r\n",
      "b\n",
      "r\n",
      "r\n",
      "r\n",
      "l\n",
      "l\n",
      "b\n",
      "r\n",
      "r\n",
      "l\n",
      "b\n",
      "l\n",
      "b\n",
      "r\n",
      "r\n",
      "r\n",
      "r\n",
      "r\n",
      "r\n",
      "b\n",
      "nan\n",
      "r\n",
      "l\n",
      "b\n",
      "r\n",
      "b\n",
      "b\n",
      "r\n",
      "l\n",
      "l\n",
      "r\n",
      "r\n",
      "r\n",
      "l\n",
      "l\n",
      "b\n",
      "b\n",
      "r\n",
      "l\n",
      "l\n",
      "b\n",
      "r\n",
      "r\n",
      "r\n",
      "r\n",
      "r\n",
      "b\n",
      "r\n",
      "l\n",
      "l\n",
      "b\n",
      "b\n",
      "b\n",
      "r\n",
      "r\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "b\n",
      "nan\n",
      "r\n",
      "r\n",
      "r\n",
      "r\n",
      "b\n",
      "l\n",
      "r\n",
      "b\n",
      "r\n",
      "r\n",
      "r\n",
      "l\n",
      "b\n",
      "r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for index, row in codf.iterrows():\n",
    "    cf = row['cofips']\n",
    "    T1 = row['T1']\n",
    "    T1 = row['T2']\n",
    "    LegendLoc = row['Legend']\n",
    "    size = row['Size']\n",
    "    \n",
    "\n",
    "    if LegendLoc == 'r':\n",
    "        LBM.visible = False\n",
    "        LBR.visible = True\n",
    "        LBL.visible = False\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  },
  "vscode": {
   "interpreter": {
    "hash": "16781b3b6eec676e6c8bb81d0e898a4b0706d5d36568974f181d7d925ef2a2cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
